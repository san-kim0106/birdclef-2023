{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Users/kimsan/Desktop/Lecture-Materials/3-1/AIGS538-Deep-Learning/final-project/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimsan/Desktop/Lecture-Materials/3-1/AIGS538-Deep-Learning/final-project/dl-final-project/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2Model, Wav2Vec2Processor, AutoProcessor\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "from librosa.feature import melspectrogram\n",
    "from PIL import Image\n",
    "from uuid import uuid4\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from tqdm import tqdm\n",
    "\n",
    "from data.dataset import DataSet\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "      <th>numeric_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/Users/kimsan/Desktop/Lecture-Materials/3-1/AI...</td>\n",
       "      <td>ruegls1</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/Users/kimsan/Desktop/Lecture-Materials/3-1/AI...</td>\n",
       "      <td>ruegls1</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/Users/kimsan/Desktop/Lecture-Materials/3-1/AI...</td>\n",
       "      <td>ruegls1</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/Users/kimsan/Desktop/Lecture-Materials/3-1/AI...</td>\n",
       "      <td>ruegls1</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/Users/kimsan/Desktop/Lecture-Materials/3-1/AI...</td>\n",
       "      <td>ruegls1</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path    label  numeric_label\n",
       "0  /Users/kimsan/Desktop/Lecture-Materials/3-1/AI...  ruegls1            188\n",
       "1  /Users/kimsan/Desktop/Lecture-Materials/3-1/AI...  ruegls1            188\n",
       "2  /Users/kimsan/Desktop/Lecture-Materials/3-1/AI...  ruegls1            188\n",
       "3  /Users/kimsan/Desktop/Lecture-Materials/3-1/AI...  ruegls1            188\n",
       "4  /Users/kimsan/Desktop/Lecture-Materials/3-1/AI...  ruegls1            188"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = pd.read_csv(\"/Users/kimsan/Desktop/Lecture-Materials/3-1/AIGS538-Deep-Learning/final-project/data/birdclef-2023/processed/trimed_df.csv\")\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wav2VecAudioClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "        \n",
    "        self.wav2vec = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "        \n",
    "        self.pooling = nn.AvgPool1d(768)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        self.projection = nn.Linear(275, 264)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, audio):\n",
    "        processed = self.processor(audio, sampling_rate=16000, return_tensors=\"pt\")[\"input_values\"]\n",
    "        processed = processed.flatten(0, 1)\n",
    "        context_representation = self.wav2vec(processed)[0]\n",
    "        pooling_state = self.pooling(context_representation)\n",
    "        pooling_state = pooling_state.view(pooling_state.shape[0], pooling_state.shape[2], pooling_state.shape[1])\n",
    "        logits = self.relu(self.projection(pooling_state))\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2Model: ['lm_head.bias', 'lm_head.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = Wav2VecAudioClassifier()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.05)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example_data, sample_rate = librosa.load(data_df.loc[0, \"path\"])\n",
    "# example_data_label = data_df.loc[0, \"label\"]\n",
    "\n",
    "# trim_data1 = example_data[: sample_rate*3]\n",
    "# trim_data2 = example_data[3*sample_rate:6*sample_rate]\n",
    "# batch_data = np.array([trim_data1, trim_data2])\n",
    "\n",
    "# one_hot_encodings = {label: index for index, label in enumerate(sorted(list(set(data_df[\"label\"]))))}\n",
    "\n",
    "# example_label_one_hot = torch.zeros(len(set(data_df[\"label\"])))\n",
    "# example_label_one_hot[one_hot_encodings[example_data_label]] = 1\n",
    "# data1_one_hot = example_label_one_hot.clone()\n",
    "# data2_one_hot = example_label_one_hot.clone()\n",
    "\n",
    "# batched_label = torch.stack((data1_one_hot, data2_one_hot))\n",
    "\n",
    "# model.train()\n",
    "# prediction = model.forward(batch_data)\n",
    "# loss = loss_fn(prediction.flatten(1, 2), batched_label)\n",
    "\n",
    "# optimizer.zero_grad()\n",
    "# loss.backward()\n",
    "# optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"batch_size\" : 128,\n",
    "    \"shuffle\"     : True,\n",
    "    \"num_workers\": 6\n",
    "}\n",
    "max_epoch = 5\n",
    "\n",
    "one_hot_encodings = {label: index for index, label in enumerate(sorted(list(set(data_df[\"label\"]))))}\n",
    "\n",
    "training_set = DataSet(data_df[\"path\"], data_df[\"label\"], one_hot_encodings)\n",
    "training_generator = torch.utils.data.DataLoader(training_set, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/260 [13:09<28:16:43, 394.59s/it]"
     ]
    }
   ],
   "source": [
    "for epoch in range(max_epoch):\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for audio, labels in tqdm(training_generator):  # Iterate over your training dataloader\n",
    "        audio = audio.to(\"cpu\")\n",
    "        labels = labels.to(\"cpu\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model.forward(audio)\n",
    "        loss = loss_fn(outputs.flatten(1, 2), labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted_labels = torch.max(outputs, dim=1)\n",
    "        total_correct += (predicted_labels == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    train_loss = total_loss / total_samples\n",
    "    train_accuracy = total_correct / total_samples\n",
    "\n",
    "    print(f\"epoch: {epoch}, train_loss: {train_loss}, train_accuracy: {train_accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
