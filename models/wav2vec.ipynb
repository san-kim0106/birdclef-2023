{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Users/kimsan/Desktop/Lecture-Materials/3-1/AIGS538-Deep-Learning/final-project/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2Model, Wav2Vec2Processor, AutoProcessor\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "from librosa.feature import melspectrogram\n",
    "from PIL import Image\n",
    "from uuid import uuid4\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy\n",
    "\n",
    "from data.dataset import DataSet\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(\"/Users/kimsan/Desktop/Lecture-Materials/3-1/AIGS538-Deep-Learning/final-project/data/birdclef-2023/train_audio/audio_paths.csv\")\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wav2VecAudioClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "        \n",
    "        self.wav2vec = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "        \n",
    "        self.pooling = nn.AvgPool1d(768)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        self.projection = nn.Linear(206, 264)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, audio):\n",
    "        processed = self.processor(audio, sampling_rate=16000, return_tensors=\"pt\")[\"input_values\"]\n",
    "        processed = processed.flatten(0, 1)\n",
    "        context_representation = self.wav2vec(processed)[0]\n",
    "        pooling_state = self.pooling(context_representation)\n",
    "        pooling_state = pooling_state.view(pooling_state.shape[0], pooling_state.shape[2], pooling_state.shape[1])\n",
    "        logits = self.relu(self.projection(pooling_state))\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Wav2VecAudioClassifier()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.05)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_data, sample_rate = librosa.load(data_df.loc[0, \"path\"])\n",
    "example_data_label = data_df.loc[0, \"label\"]\n",
    "\n",
    "trim_data1 = example_data[: sample_rate*3]\n",
    "trim_data2 = example_data[3*sample_rate:6*sample_rate]\n",
    "batch_data = np.array([trim_data1, trim_data2])\n",
    "\n",
    "one_hot_encodings = {label: index for index, label in enumerate(sorted(list(set(data_df[\"label\"]))))}\n",
    "\n",
    "example_label_one_hot = torch.zeros(len(set(data_df[\"label\"])))\n",
    "example_label_one_hot[one_hot_encodings[example_data_label]] = 1\n",
    "data1_one_hot = example_label_one_hot.clone()\n",
    "data2_one_hot = example_label_one_hot.clone()\n",
    "\n",
    "batched_label = torch.stack((data1_one_hot, data2_one_hot))\n",
    "\n",
    "model.train()\n",
    "prediction = model.forward(batch_data)\n",
    "loss = loss_fn(prediction.flatten(1, 2), batched_label)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"batch_size\" : 64,\n",
    "    \"shuffle\"     : True,\n",
    "    \"num_workers\": 6\n",
    "}\n",
    "max_epoch = 100\n",
    "\n",
    "one_hot_encodings = {label: index for index, label in enumerate(sorted(list(set(data_df[\"label\"]))))}\n",
    "\n",
    "training_set = DataSet(data_df[\"path\"], data_df[\"label\"], one_hot_encodings)\n",
    "training_generator = torch.utils.data.DataLoader(training_set, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(max_epoch):\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for audio, labels in training_generator:  # Iterate over your training dataloader\n",
    "        print(labels)\n",
    "        # audio = preprocess_audio(audio)\n",
    "        # labels = preprocess_labels(labels)\n",
    "        # audio = audio.to(\"cpu\")\n",
    "        # labels = labels.to(\"cpu\")\n",
    "\n",
    "        # optimizer.zero_grad()\n",
    "        # outputs = model(input_values=audio, labels=labels)\n",
    "        # loss = outputs.loss\n",
    "        # logits = outputs.logits\n",
    "        # loss.backward()\n",
    "        # optimizer.step()\n",
    "\n",
    "        # total_loss += loss.item()\n",
    "        # _, predicted_labels = torch.max(logits, dim=1)\n",
    "        # total_correct += (predicted_labels == labels).sum().item()\n",
    "        # total_samples += labels.size(0)\n",
    "\n",
    "    train_loss = total_loss / total_samples\n",
    "    train_accuracy = total_correct / total_samples\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
